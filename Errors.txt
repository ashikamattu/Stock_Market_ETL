DATAFLOW ERROR - 
"java.lang.RuntimeException: Failed to create job with prefix beam_load_csvtobigqueryxxxxxxxxxxxxxx, reached max retries: 3, last failed job: null. at org.apache.beam.sdk.io.gcp.bigquery.BigQueryHelpers$PendingJob.runJob(BigQueryHelpers.java:198)..... "

One of the possible cause could be the privilege issue. Ensure the user account which interacts with the BigQuery has privilege "bigquery.jobs.create" in the predefined role "*BigQuery User"

Steps - 
IAM & Admin-> Roles -> Create custom role "RoleBQueryAdmin"-> BigQuery Admin -> bigquery.jobs.create among many

Then go to IAM -> In Project Permissions setting -> Add "RoleBQueryAdmin" to the service/user account through which the project is interacting. 

Remove spaces from input file name 

Check schema - There is no datatype called BIGINT in Bquery. Alternate is BIGNUMERIC

{ "BigQuery Schema": [ 
        { "name": "ticker", "type": "STRING" }, 
        { "name": "current_price", "type": "FLOAT" }, 
        { "name": "total_revenue", "type": "BIGNUMERIC" }, 
        { "name": "ebitda", "type": "BIGNUMERIC" }, 
        { "name": "free_cash_flow", "type": "BIGNUMERIC" },
        { "name": "profit_margins", "type": "FLOAT" }, 
        { "name": "revenue_growth", "type": "FLOAT" }, 
        { "name": "debt_to_equity", "type": "FLOAT" }, 
        { "name": "total_debt", "type": "BIGNUMERIC" }, 
        { "name": "num_analyst_opinions", "type": "INTEGER" },
        { "name": "recommendation_key", "type": "STRING" }, 
        { "name": "time_stamp", "type": "TIMESTAMP" }
    ] 
}

IAM Access required ref doc - https://cloud.google.com/bigquery/docs/batch-loading-data


Data transformation - 
import json
def process(value):
  data = value.split(',')
  obj = { 'name': data[0], 'age': int(data[1]) }
  return json.dumps(obj)